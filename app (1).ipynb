{"cells":[{"cell_type":"code","source":["# ========== 0. Install dependencies ==========\n","!pip install -q bitsandbytes transformers accelerate peft datasets torch sentencepiece \\\n","    sentence-transformers faiss-cpu evaluate rouge_score\n","!pip install langdetect"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w8yeni3eKmMa","executionInfo":{"status":"ok","timestamp":1762418888992,"user_tz":-330,"elapsed":37848,"user":{"displayName":"RevenueRoot","userId":"09989107166483880833"}},"outputId":"63129414-8aea-4607-fe84-a6c1d7ff3129"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: langdetect in /usr/local/lib/python3.12/dist-packages (1.0.9)\n","Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from langdetect) (1.17.0)\n"]}]},{"cell_type":"code","source":["# ========== 1. Mount Drive ==========\n","from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T1NTiB0Ybai3","executionInfo":{"status":"ok","timestamp":1762418910648,"user_tz":-330,"elapsed":4208,"user":{"displayName":"RevenueRoot","userId":"09989107166483880833"}},"outputId":"f999b6b1-ddac-45dd-bcf5-8971f2da8956"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# ========== 2. Paths ==========\n","import os\n","BASE_DIR = \"/content/drive/MyDrive/guvi_chatbot_project\"\n","DATA_PATH = os.path.join(BASE_DIR, \"data\", \"guvi_course_info_io.jsonl\")\n","ADAPTER_DIR = os.path.join(BASE_DIR, \"adapters/final\")\n","CHECKPOINT_DIR = os.path.join(BASE_DIR, \"checkpoints\")\n","HF_CACHE = os.path.join(BASE_DIR, \"base_model\", \"hf_cache\")\n","\n","os.makedirs(ADAPTER_DIR, exist_ok=True)\n","os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n","os.makedirs(HF_CACHE, exist_ok=True)"],"metadata":{"id":"PJDYVB4VbamF","executionInfo":{"status":"ok","timestamp":1762418913552,"user_tz":-330,"elapsed":34,"user":{"displayName":"RevenueRoot","userId":"09989107166483880833"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# ========== 3. Load base model & tokenizer ==========\n","from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, BitsAndBytesConfig, pipeline as hf_pipeline\n","MODEL_ID = \"MBZUAI/LaMini-Flan-T5-783M\"\n","bnb_config = BitsAndBytesConfig(load_in_8bit=True)\n","\n","print(\"Loading model (8-bit)...\")\n","model = AutoModelForSeq2SeqLM.from_pretrained(\n","    MODEL_ID, device_map=\"auto\", quantization_config=bnb_config, cache_dir=HF_CACHE\n",")\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, cache_dir=HF_CACHE)\n","print(\"Model + tokenizer loaded.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3WwwinNbbao8","executionInfo":{"status":"ok","timestamp":1762419027096,"user_tz":-330,"elapsed":111337,"user":{"displayName":"RevenueRoot","userId":"09989107166483880833"}},"outputId":"cf04b503-9491-4bc4-b364-159c9dd11bf3"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading model (8-bit)...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["Model + tokenizer loaded.\n"]}]},{"cell_type":"code","source":["# ========== 4. Translator Model ==========\n","NLLB = \"facebook/nllb-200-distilled-600M\"\n","print(\"Loading NLLB translation model and tokenizer...\")\n","nllb_tokenizer = AutoTokenizer.from_pretrained(NLLB, cache_dir=HF_CACHE)\n","nllb_model = AutoModelForSeq2SeqLM.from_pretrained(NLLB, device_map=\"auto\", cache_dir=HF_CACHE)\n","\n","# build a pipeline wrapper (we will pass src_lang / tgt_lang per call)\n","translator = hf_pipeline(\"translation\", model=nllb_model, tokenizer=nllb_tokenizer)\n","print(\"Translator ready.\")\n","\n","# Simple language detection + mapping to NLLB codes:\n","from langdetect import detect, DetectorFactory\n","DetectorFactory.seed = 0 # make detection deterministic\n","LANG_MAP = {\n","    \"en\": \"eng_Latn\",\n","    \"hi\": \"hin_Deva\",\n","    \"bn\": \"ben_Beng\",\n","    \"te\": \"tel_Telu\",\n","    \"ta\": \"tam_Taml\",\n","    \"mr\": \"mar_Deva\",\n","    \"gu\": \"guj_Gujr\",\n","    \"pa\": \"pan_Guru\",\n","    \"or\": \"ori_Orya\",\n","    \"kn\": \"kan_Knda\",\n","    \"ml\": \"mal_Mlym\",\n","    \"ur\": \"urd_Arab\",\n","    \"fr\": \"fra_Latn\",\n","    \"es\": \"spa_Latn\",\n","    \"de\": \"deu_Latn\",\n","    \"ru\": \"rus_Cyrl\",\n","    \"zh-cn\": \"zho_Hans\",\n","    \"zh\": \"zho_Hans\",\n","    \"ar\": \"ara_Arab\",\n","    \"pt\": \"por_Latn\" }\n","\n","def detect_lang(text):\n","  try:\n","    code = detect(text.lower().strip())\n","    return code if code in LANG_MAP else None\n","  except Exception:\n","    return None\n","\n","def langdetect_to_nllb(code):\n","  if not code: return None\n","  return LANG_MAP.get(code, LANG_MAP.get(\"en\"))\n","\n","def clean_text(text):\n","  \"\"\"Normalize text before translation for better fidelity.\"\"\"\n","  text = text.strip()\n","  text = \" \".join(text.split())\n","  return text\n","\n","def translate_text(text, src, tgt, max_length=1024):\n","  \"\"\"Robust translation using NLLB with batching and normalization.\"\"\"\n","  text = clean_text(text)\n","  if src == tgt or not src or not tgt:\n","      return text\n","  try:\n","      result = translator(text, src_lang=src, tgt_lang=tgt, max_length=max_length)\n","      if isinstance(result, list) and len(result) > 0:\n","        return result[0].get(\"translation_text\", text)\n","      return str(result)\n","  except Exception as e:\n","      print(f\"[WARN] Translation failed ({src}->{tgt}):\", e)\n","      return text"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ox_H9avMbasE","executionInfo":{"status":"ok","timestamp":1762419538124,"user_tz":-330,"elapsed":37443,"user":{"displayName":"RevenueRoot","userId":"09989107166483880833"}},"outputId":"e5ba8daf-a8f1-43ed-b655-ed595acbc02f"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading NLLB translation model and tokenizer...\n"]},{"output_type":"stream","name":"stderr","text":["Device set to use cuda:0\n"]},{"output_type":"stream","name":"stdout","text":["Translator ready.\n"]}]},{"cell_type":"code","source":["# ========== 5. Load dataset and split ==========\n","from datasets import load_dataset, DatasetDict\n","print(\"Loading dataset:\", DATA_PATH)\n","raw = load_dataset(\"json\", data_files=DATA_PATH, split=\"train\")\n","print(\"Dataset size:\", len(raw))\n","\n","# Shuffle and split into train/validation (90/10)\n","raw = raw.shuffle(seed=42)\n","split = raw.train_test_split(test_size=0.10, seed=42)\n","train_ds = split[\"train\"]\n","eval_ds = split[\"test\"]\n","print(\"Train / Eval sizes:\", len(train_ds), len(eval_ds))\n","\n","# Quick peek (ensure your jsonl has 'input' and 'output' keys)\n","print(\"Columns:\", train_ds.column_names)\n","print(\"Example:\", train_ds[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3R7F36wrbau5","executionInfo":{"status":"ok","timestamp":1762419564087,"user_tz":-330,"elapsed":1245,"user":{"displayName":"RevenueRoot","userId":"09989107166483880833"}},"outputId":"c4f0498b-5b94-48bb-8153-d6815f6bf6c9"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading dataset: /content/drive/MyDrive/guvi_chatbot_project/data/guvi_course_info_io.jsonl\n","Dataset size: 119\n","Train / Eval sizes: 107 12\n","Columns: ['input', 'output']\n","Example: {'input': 'Answer as a GUVI course assistant: What topics are in the free C course?', 'output': 'It covers Operators, Loops, File operations, and Preprocessor directives. Free to learn; fee for GUVI Certification. Visit guvi.in for more details.'}\n"]}]},{"cell_type":"code","source":["# ========== 6. Prompt format used for training ==========\n","\n","def make_training_prompt(question, context=\"\"):\n","    # keep format consistent for both train and inference\n","    if context:\n","        return f\"Context: {context}\\nQuestion: {question}\\nAnswer:\"\n","    return f\"Question: {question}\\nAnswer:\""],"metadata":{"id":"lv60itj1baxr","executionInfo":{"status":"ok","timestamp":1762419567662,"user_tz":-330,"elapsed":6,"user":{"displayName":"RevenueRoot","userId":"09989107166483880833"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# ========== 7. Tokenization (use collator for dynamic padding) ==========\n","MAX_SOURCE_LENGTH = 256\n","MAX_TARGET_LENGTH = 128\n","\n","def preprocess_fn(batch):\n","    questions = batch[\"input\"]\n","    answers = batch[\"output\"]\n","\n","    inputs = [make_training_prompt(q, \"\") for q in questions]\n","    model_inputs = tokenizer(\n","        inputs,\n","        truncation=True,\n","        max_length=MAX_SOURCE_LENGTH,\n","        padding=\"max_length\",\n","        return_tensors=\"pt\"\n","    )\n","\n","    with tokenizer.as_target_tokenizer():\n","        labels = tokenizer(\n","            answers,\n","            truncation=True,\n","            padding=\"max_length\",\n","            max_length=MAX_TARGET_LENGTH,\n","            return_tensors=\"pt\"\n","        )[\"input_ids\"]\n","\n","    # Replace padding token ids with -100\n","    labels_masked = [\n","        [(l if l != tokenizer.pad_token_id else -100) for l in label]\n","        for label in labels.tolist()\n","    ]\n","    model_inputs[\"labels\"] = labels_masked\n","\n","    # Safely convert all tensors to lists\n","    processed = {}\n","    for k, v in model_inputs.items():\n","        if hasattr(v, \"tolist\"):\n","            processed[k] = v.tolist()\n","        else:\n","            processed[k] = v\n","\n","    return processed\n","\n","# Apply tokenization\n","train_tok = train_ds.map(preprocess_fn, batched=True, remove_columns=train_ds.column_names)\n","eval_tok = eval_ds.map(preprocess_fn, batched=True, remove_columns=eval_ds.column_names)\n","print(\"Tokenized shapes example:\", {k: len(train_tok[0][k]) for k in train_tok.column_names})\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NwLmtYZKba0n","executionInfo":{"status":"ok","timestamp":1762419570380,"user_tz":-330,"elapsed":62,"user":{"displayName":"RevenueRoot","userId":"09989107166483880833"}},"outputId":"165bcb05-dfc1-4868-8806-d73c4ce5df22"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenized shapes example: {'input_ids': 256, 'attention_mask': 256, 'labels': 128}\n"]}]},{"cell_type":"code","source":["# ========== 8. LoRA / PEFT setup (optional but you had it) ==========\n","from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n","model = prepare_model_for_kbit_training(model)\n","\n","peft_config = LoraConfig(\n","    r=8,\n","    lora_alpha=16,\n","    target_modules=[\"q\", \"k\", \"v\", \"o\"],\n","    lora_dropout=0.1,\n","    bias=\"none\",\n","    task_type=\"SEQ_2_SEQ_LM\"\n",")\n","model = get_peft_model(model, peft_config)\n","model.print_trainable_parameters()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u_AjPDERba3q","executionInfo":{"status":"ok","timestamp":1762419575648,"user_tz":-330,"elapsed":713,"user":{"displayName":"RevenueRoot","userId":"09989107166483880833"}},"outputId":"358b070d-b59e-4c64-e9da-02b663ef5219"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["trainable params: 4,718,592 || all params: 787,868,672 || trainable%: 0.5989\n"]}]},{"cell_type":"code","source":["# ========== 9. Training args & trainer ==========\n","from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer, DataCollatorForSeq2Seq\n","import evaluate\n","rouge = evaluate.load(\"rouge\")\n","\n","training_args = Seq2SeqTrainingArguments(\n","    output_dir=CHECKPOINT_DIR,\n","    per_device_train_batch_size=2,\n","    gradient_accumulation_steps=4,\n","    eval_accumulation_steps=2,\n","    eval_strategy=\"steps\",\n","    eval_steps=200,\n","    save_strategy=\"steps\",\n","    save_steps=200,\n","    logging_steps=20,\n","    num_train_epochs=3,\n","    learning_rate=2e-5,\n","    fp16=True,\n","    predict_with_generate=True,\n","    load_best_model_at_end=True,\n","    metric_for_best_model=\"rougeL\",\n","    greater_is_better=True,\n","    save_total_limit=3,\n","    push_to_hub=False,\n",")\n","\n","data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=\"longest\", return_tensors=\"pt\")\n","\n","def postprocess_text(preds, labels):\n","    preds = [p.strip() for p in preds]\n","    labels = [l.strip() for l in labels]\n","    return preds, labels\n","\n","def compute_metrics(eval_preds):\n","    preds, labels = eval_preds\n","    if isinstance(preds, tuple):\n","        preds = preds[0]\n","    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n","    # labels come as token ids with -100 for pad\n","    labels = [[(l if l != -100 else tokenizer.pad_token_id) for l in label] for label in labels]\n","    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n","    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n","    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n","    # convert to percentages\n","    result = {k: float(v.mid.fmeasure * 100) for k, v in result.items()}\n","    # add avg length\n","    prediction_lens = [len(tokenizer.encode(p)) for p in decoded_preds]\n","    result[\"gen_len\"] = sum(prediction_lens) / max(1, len(prediction_lens))\n","    return result\n","\n","from transformers import set_seed\n","set_seed(42)\n","\n","trainer = Seq2SeqTrainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_tok,\n","    eval_dataset=eval_tok,\n","    data_collator=data_collator,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics\n",")\n","\n","# Turn off WANDB in colab to keep logs local\n","import os\n","os.environ[\"WANDB_MODE\"] = \"disabled\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AWrws4Ggba6w","executionInfo":{"status":"ok","timestamp":1762419583299,"user_tz":-330,"elapsed":4739,"user":{"displayName":"RevenueRoot","userId":"09989107166483880833"}},"outputId":"be7c8bd9-faa2-4030-9989-99bd53e7393f"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-4104284812.py:54: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n","  trainer = Seq2SeqTrainer(\n"]}]},{"cell_type":"code","source":["# ========== 10. Train ==========\n","trainer.train()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":234},"id":"ME0GGddxKmPI","executionInfo":{"status":"ok","timestamp":1762419800215,"user_tz":-330,"elapsed":201806,"user":{"displayName":"RevenueRoot","userId":"09989107166483880833"}},"outputId":"f2188607-aa24-4a33-b36d-cf10f6fddef3"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:929: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n","  return fn(*args, **kwargs)\n","/usr/local/lib/python3.12/dist-packages/bitsandbytes/autograd/_functions.py:181: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n","  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n","`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='42' max='42' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [42/42 03:12, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=42, training_loss=0.0, metrics={'train_runtime': 201.0592, 'train_samples_per_second': 1.597, 'train_steps_per_second': 0.209, 'total_flos': 372241786208256.0, 'train_loss': 0.0, 'epoch': 3.0})"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["# ========== 11. Save adapter + tokenizer ==========\n","model.save_pretrained(ADAPTER_DIR)\n","tokenizer.save_pretrained(ADAPTER_DIR)\n","print(\"Saved adapter & tokenizer to:\", ADAPTER_DIR)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XxAL1k4QcNxp","executionInfo":{"status":"ok","timestamp":1762419827960,"user_tz":-330,"elapsed":516,"user":{"displayName":"RevenueRoot","userId":"09989107166483880833"}},"outputId":"1b41ae3e-f3ef-40df-ac8c-7ebce2aca33c"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Saved adapter & tokenizer to: /content/drive/MyDrive/guvi_chatbot_project/adapters/final\n"]}]},{"cell_type":"code","source":["# ========== 12. Build retrieval (FAISS) safely ==========\n","USE_RETRIEVAL = True\n","if USE_RETRIEVAL:\n","    from sentence_transformers import SentenceTransformer\n","    import faiss, numpy as np\n","    embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n","    # corpus: preferably use a cleaned short context field, not full pair\n","    corpus_texts = [d[\"input\"] + \" || \" + d.get(\"output\", \"\") for d in train_ds]\n","    embeddings = embed_model.encode(corpus_texts, convert_to_numpy=True, show_progress_bar=True)\n","    faiss.normalize_L2(embeddings)\n","    d = embeddings.shape[1]\n","    index = faiss.IndexFlatIP(d)\n","    index.add(embeddings)\n","    faiss.write_index(index, os.path.join(BASE_DIR, \"faiss_index.idx\"))\n","    print(\"Built FAISS index with\", index.ntotal, \"vectors.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":66,"referenced_widgets":["0dbb0c7778204cc5b6eae10ed6e32f56","febdf61c8a2b4cbfa377d2e3075b040d","85b7e2b5ce6f4de3a1e4bdfadcc3bf35","8ecc5f48b5fd4d0ab8270f977b7476ac","254ec91c19384f29b70d5cb0f95ab6f5","f28d72e6e40c478da77a925e09401161","39d721582a4244729fcf80d7d9fb6225","d253f1b29fd74d55be909f5f380686e8","2d47787269674f7ca80f3a8694d1e1e0","8204ea343f7446228f0a45dea44605dc","6c13b891439d459a92fe70c2869c9013"]},"id":"ErHrybTgcNuo","executionInfo":{"status":"ok","timestamp":1762419832937,"user_tz":-330,"elapsed":2569,"user":{"displayName":"RevenueRoot","userId":"09989107166483880833"}},"outputId":"641b8e51-22b6-4e97-fd84-0520aaa38caf"},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":["Batches:   0%|          | 0/4 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0dbb0c7778204cc5b6eae10ed6e32f56"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Built FAISS index with 107 vectors.\n"]}]},{"cell_type":"code","source":["# ========== 13. Reload model + adapter (inference-ready) ==========\n","from peft import PeftModel\n","model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_ID, quantization_config=bnb_config, device_map=\"auto\", cache_dir=HF_CACHE)\n","model = PeftModel.from_pretrained(model, ADAPTER_DIR)  # loads LoRA adapter\n","tokenizer = AutoTokenizer.from_pretrained(ADAPTER_DIR, cache_dir=HF_CACHE)\n","model.eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1Py6IJvmcNrN","executionInfo":{"status":"ok","timestamp":1762419878990,"user_tz":-330,"elapsed":43043,"user":{"displayName":"RevenueRoot","userId":"09989107166483880833"}},"outputId":"fd7f33f2-3cd1-44b8-b053-dba714789c86"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["PeftModelForSeq2SeqLM(\n","  (base_model): LoraModel(\n","    (model): T5ForConditionalGeneration(\n","      (shared): Embedding(32128, 1024)\n","      (encoder): T5Stack(\n","        (embed_tokens): Embedding(32128, 1024)\n","        (block): ModuleList(\n","          (0): T5Block(\n","            (layer): ModuleList(\n","              (0): T5LayerSelfAttention(\n","                (SelfAttention): T5Attention(\n","                  (q): lora.Linear8bitLt(\n","                    (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=1024, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=1024, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                    (lora_magnitude_vector): ModuleDict()\n","                  )\n","                  (k): lora.Linear8bitLt(\n","                    (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=1024, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=1024, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                    (lora_magnitude_vector): ModuleDict()\n","                  )\n","                  (v): lora.Linear8bitLt(\n","                    (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=1024, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=1024, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                    (lora_magnitude_vector): ModuleDict()\n","                  )\n","                  (o): lora.Linear8bitLt(\n","                    (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=1024, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=1024, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                    (lora_magnitude_vector): ModuleDict()\n","                  )\n","                  (relative_attention_bias): Embedding(32, 16)\n","                )\n","                (layer_norm): T5LayerNorm()\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (1): T5LayerFF(\n","                (DenseReluDense): T5DenseGatedActDense(\n","                  (wi_0): Linear8bitLt(in_features=1024, out_features=2816, bias=False)\n","                  (wi_1): Linear8bitLt(in_features=1024, out_features=2816, bias=False)\n","                  (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                  (act): NewGELUActivation()\n","                )\n","                (layer_norm): T5LayerNorm()\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","          )\n","          (1-23): 23 x T5Block(\n","            (layer): ModuleList(\n","              (0): T5LayerSelfAttention(\n","                (SelfAttention): T5Attention(\n","                  (q): lora.Linear8bitLt(\n","                    (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=1024, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=1024, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                    (lora_magnitude_vector): ModuleDict()\n","                  )\n","                  (k): lora.Linear8bitLt(\n","                    (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=1024, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=1024, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                    (lora_magnitude_vector): ModuleDict()\n","                  )\n","                  (v): lora.Linear8bitLt(\n","                    (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=1024, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=1024, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                    (lora_magnitude_vector): ModuleDict()\n","                  )\n","                  (o): lora.Linear8bitLt(\n","                    (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=1024, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=1024, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                    (lora_magnitude_vector): ModuleDict()\n","                  )\n","                )\n","                (layer_norm): T5LayerNorm()\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (1): T5LayerFF(\n","                (DenseReluDense): T5DenseGatedActDense(\n","                  (wi_0): Linear8bitLt(in_features=1024, out_features=2816, bias=False)\n","                  (wi_1): Linear8bitLt(in_features=1024, out_features=2816, bias=False)\n","                  (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                  (act): NewGELUActivation()\n","                )\n","                (layer_norm): T5LayerNorm()\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","          )\n","        )\n","        (final_layer_norm): T5LayerNorm()\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (decoder): T5Stack(\n","        (embed_tokens): Embedding(32128, 1024)\n","        (block): ModuleList(\n","          (0): T5Block(\n","            (layer): ModuleList(\n","              (0): T5LayerSelfAttention(\n","                (SelfAttention): T5Attention(\n","                  (q): lora.Linear8bitLt(\n","                    (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=1024, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=1024, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                    (lora_magnitude_vector): ModuleDict()\n","                  )\n","                  (k): lora.Linear8bitLt(\n","                    (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=1024, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=1024, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                    (lora_magnitude_vector): ModuleDict()\n","                  )\n","                  (v): lora.Linear8bitLt(\n","                    (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=1024, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=1024, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                    (lora_magnitude_vector): ModuleDict()\n","                  )\n","                  (o): lora.Linear8bitLt(\n","                    (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=1024, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=1024, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                    (lora_magnitude_vector): ModuleDict()\n","                  )\n","                  (relative_attention_bias): Embedding(32, 16)\n","                )\n","                (layer_norm): T5LayerNorm()\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (1): T5LayerCrossAttention(\n","                (EncDecAttention): T5Attention(\n","                  (q): lora.Linear8bitLt(\n","                    (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=1024, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=1024, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                    (lora_magnitude_vector): ModuleDict()\n","                  )\n","                  (k): lora.Linear8bitLt(\n","                    (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=1024, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=1024, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                    (lora_magnitude_vector): ModuleDict()\n","                  )\n","                  (v): lora.Linear8bitLt(\n","                    (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=1024, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=1024, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                    (lora_magnitude_vector): ModuleDict()\n","                  )\n","                  (o): lora.Linear8bitLt(\n","                    (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=1024, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=1024, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                    (lora_magnitude_vector): ModuleDict()\n","                  )\n","                )\n","                (layer_norm): T5LayerNorm()\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (2): T5LayerFF(\n","                (DenseReluDense): T5DenseGatedActDense(\n","                  (wi_0): Linear8bitLt(in_features=1024, out_features=2816, bias=False)\n","                  (wi_1): Linear8bitLt(in_features=1024, out_features=2816, bias=False)\n","                  (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                  (act): NewGELUActivation()\n","                )\n","                (layer_norm): T5LayerNorm()\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","          )\n","          (1-23): 23 x T5Block(\n","            (layer): ModuleList(\n","              (0): T5LayerSelfAttention(\n","                (SelfAttention): T5Attention(\n","                  (q): lora.Linear8bitLt(\n","                    (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=1024, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=1024, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                    (lora_magnitude_vector): ModuleDict()\n","                  )\n","                  (k): lora.Linear8bitLt(\n","                    (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=1024, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=1024, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                    (lora_magnitude_vector): ModuleDict()\n","                  )\n","                  (v): lora.Linear8bitLt(\n","                    (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=1024, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=1024, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                    (lora_magnitude_vector): ModuleDict()\n","                  )\n","                  (o): lora.Linear8bitLt(\n","                    (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=1024, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=1024, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                    (lora_magnitude_vector): ModuleDict()\n","                  )\n","                )\n","                (layer_norm): T5LayerNorm()\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (1): T5LayerCrossAttention(\n","                (EncDecAttention): T5Attention(\n","                  (q): lora.Linear8bitLt(\n","                    (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=1024, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=1024, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                    (lora_magnitude_vector): ModuleDict()\n","                  )\n","                  (k): lora.Linear8bitLt(\n","                    (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=1024, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=1024, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                    (lora_magnitude_vector): ModuleDict()\n","                  )\n","                  (v): lora.Linear8bitLt(\n","                    (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=1024, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=1024, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                    (lora_magnitude_vector): ModuleDict()\n","                  )\n","                  (o): lora.Linear8bitLt(\n","                    (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n","                    (lora_dropout): ModuleDict(\n","                      (default): Dropout(p=0.1, inplace=False)\n","                    )\n","                    (lora_A): ModuleDict(\n","                      (default): Linear(in_features=1024, out_features=8, bias=False)\n","                    )\n","                    (lora_B): ModuleDict(\n","                      (default): Linear(in_features=8, out_features=1024, bias=False)\n","                    )\n","                    (lora_embedding_A): ParameterDict()\n","                    (lora_embedding_B): ParameterDict()\n","                    (lora_magnitude_vector): ModuleDict()\n","                  )\n","                )\n","                (layer_norm): T5LayerNorm()\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","              (2): T5LayerFF(\n","                (DenseReluDense): T5DenseGatedActDense(\n","                  (wi_0): Linear8bitLt(in_features=1024, out_features=2816, bias=False)\n","                  (wi_1): Linear8bitLt(in_features=1024, out_features=2816, bias=False)\n","                  (wo): Linear(in_features=2816, out_features=1024, bias=False)\n","                  (dropout): Dropout(p=0.1, inplace=False)\n","                  (act): NewGELUActivation()\n","                )\n","                (layer_norm): T5LayerNorm()\n","                (dropout): Dropout(p=0.1, inplace=False)\n","              )\n","            )\n","          )\n","        )\n","        (final_layer_norm): T5LayerNorm()\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","      (lm_head): Linear(in_features=1024, out_features=32128, bias=False)\n","    )\n","  )\n",")"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["# ========== 14. Inference function with retrieval guard + multilingual wrapper ==========\n","import numpy as np\n","\n","def generate_response_english(prompt, use_retrieval=True, top_k=2, max_new_tokens=120, num_beams=4, temperature=0.1, top_p=0.95):\n","\n","    context_prefix = \"\"\n","    if USE_RETRIEVAL and use_retrieval:\n","        q_emb = embed_model.encode([prompt], convert_to_numpy=True)\n","        faiss.normalize_L2(q_emb)\n","        D, I = index.search(q_emb, top_k)\n","        retrieved = []\n","        for score, idx in zip(D[0], I[0]):\n","            if idx < len(corpus_texts) and score > 0.2:\n","                retrieved.append(corpus_texts[idx])\n","        if retrieved:\n","            context_prefix = \"Context: \" + \" || \".join(retrieved[:2]) + \"\\n\\n\"\n","    full_prompt = (context_prefix + \"Question: \" + prompt + \"\\nAnswer:\").strip()\n","    inputs = tokenizer(full_prompt, return_tensors=\"pt\", truncation=True, max_length=MAX_SOURCE_LENGTH).to(model.device)\n","    outputs = model.generate(\n","        **inputs,\n","        max_new_tokens=max_new_tokens,\n","        num_beams=num_beams,\n","        early_stopping=True,\n","        length_penalty=1.0,\n","        no_repeat_ngram_size=3,\n","        temperature=temperature,\n","        top_p=top_p,\n","        pad_token_id=tokenizer.pad_token_id,\n","        eos_token_id=tokenizer.eos_token_id\n","    )\n","    resp = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","    if \"Answer:\" in resp:\n","        resp = resp.split(\"Answer:\")[-1].strip()\n","    return resp\n","\n","\n","def generate_multilingual_response(user_text, user_specified_lang=None, use_retrieval=True, **kwargs):\n","\n","    # Step 1: Detect language\n","    detected = user_specified_lang or detect_lang(user_text)\n","    src_nllb = langdetect_to_nllb(detected)\n","    if not src_nllb:\n","      src_nllb = \"eng_Latn\"\n","      detected = \"en\"\n","\n","    # Step 2: Translate input → English if not already English\n","    if src_nllb != \"eng_Latn\":\n","      english_prompt = translate_text(user_text, src_nllb, \"eng_Latn\")\n","    else:\n","      english_prompt = user_text\n","\n","    # Step 3: Generate English answer\n","    english_reply = generate_response_english(\n","        english_prompt,\n","        use_retrieval=use_retrieval,\n","        num_beams=4,\n","        temperature=0.1, # small creative boost improves translation back\n","        **kwargs )\n","\n","    # Step 4: Translate English answer → user language\n","    if src_nllb != \"eng_Latn\":\n","      final_reply = translate_text(english_reply, \"eng_Latn\", src_nllb)\n","    else:\n","      final_reply = english_reply\n","\n","    return final_reply.strip(), detected\n","\n"],"metadata":{"id":"oOXToSN5cNoD","executionInfo":{"status":"ok","timestamp":1762419885181,"user_tz":-330,"elapsed":10,"user":{"displayName":"RevenueRoot","userId":"09989107166483880833"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# ========== 15. Quick local checks on validation set (multilingual) ==========\n","for i in range(5):\n","    sample = eval_ds[i][\"input\"]\n","    print(\"Q:\", sample)\n","    print(\"GT:\", eval_ds[i].get(\"output\", \"\"))\n","    pred, det = generate_multilingual_response(sample)\n","    print(\"PRED (detected_lang={}):\".format(det), pred)\n","    print(\"-\" * 60)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-mXYUgobcNaI","executionInfo":{"status":"ok","timestamp":1762415517287,"user_tz":-330,"elapsed":37322,"user":{"displayName":"RevenueRoot","userId":"09989107166483880833"}},"outputId":"87366b03-4ba5-4bb2-8322-dd4d8dde3878"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Q: Answer as a GUVI course assistant: What free courses does GUVI offer?\n","GT: GUVI offers free courses including Python Programming, C Programming, Data Science & Analytics, Ethical Hacking, and Introduction to Cloud Computing. There are many more free courses available. Visit guvi.in for more details.\n"]},{"output_type":"stream","name":"stderr","text":["The following generation flags are not valid and may be ignored: ['temperature', 'top_p']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"]},{"output_type":"stream","name":"stdout","text":["PRED (detected_lang=en): No, this question is not answerable as it is asking about the free courses offered by GUVI, while the given context is about the paid courses offered.\n","------------------------------------------------------------\n","Q: Answer as a GUVI course assistant: How do I contact GUVI support?\n","GT: You can write to cs@guvi.in or call +91 97360 97320. You can also visit the 'Contact Us' page on guvi.in. Visit guvi.in for more details.\n","PRED (detected_lang=en): To contact GUVI support, visit guvi.in for full syllabus, pricing, tools, and placement support details.\n","------------------------------------------------------------\n","Q: Answer as a GUVI course assistant: Tell me about the Java Full-stack Development program.\n","GT: This is a 3-month (weekdays) or 5-month (weekends) professional full-stack development program with placement support. It’s available in English. You’ll learn Java, HTML, CSS, JavaScript, Spring, MySQL, MongoDB, AWS, Git, Maven, JUnit, Bootstrap, and Eclipse. You’ll earn an Industry-Recognized Certification from HCL GUVI. It covers industry-relevant tools and real-world projects. Visit guvi.in for full syllabus, pricing, tools, and placement support details.\n","PRED (detected_lang=en): The Java Full-stack Development program covers Java, HTML, CSS, JavaScript, Spring, MySQL, MongoDB, AWS, Git, Maven, JUnit, Bootstrap, and Eclipse. It covers industry-relevant tools and real-world projects. Visit guvi.in for full syllabus, pricing, tools, and placement support details.\n","------------------------------------------------------------\n","Q: Answer as a GUVI course assistant: What tools are covered in the DevOps & Cloud Engineering program?\n","GT: The program covers Git, Jenkins, Docker, Kubernetes, AWS, Terraform, Ansible, Linux, Prometheus, and Grafana. It covers industry-relevant tools and real-world projects. Visit guvi.in for full syllabus, pricing, tools, and placement support details.\n","PRED (detected_lang=en): Yes, the DevOps & Cloud Engineering program is available in Hindi.\n","------------------------------------------------------------\n","Q: Answer as a GUVI course assistant: Does the DSA bundle include certification?\n","GT: Yes, you earn a GUVI HCL certification for each course in the bundle. Visit guvi.in for more details.\n","PRED (detected_lang=en): Yes, you earn a GUVI HCL certification for each course in the DSA bundle.\n","------------------------------------------------------------\n"]}]},{"cell_type":"code","source":["tests = [\n","    \"ಡೇಟಾ ಸೈನ್ಸ್ ಕೋರ್ಸ್‌ನ ಅವಧಿ ಎಷ್ಟು?\"\n","]\n","\n","for t in tests:\n","    resp, det = generate_multilingual_response(t)\n","    print(f\"\\nPrompt ({det}): {t}\")\n","    print(\"Response:\", resp)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ewh63CrBKmSJ","executionInfo":{"status":"ok","timestamp":1762420112817,"user_tz":-330,"elapsed":9582,"user":{"displayName":"RevenueRoot","userId":"09989107166483880833"}},"outputId":"901cb329-8df7-4e0f-dec8-902a02c9d631"},"execution_count":24,"outputs":[{"output_type":"stream","name":"stderr","text":["You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"]},{"output_type":"stream","name":"stdout","text":["\n","Prompt (kn): ಡೇಟಾ ಸೈನ್ಸ್ ಕೋರ್ಸ್‌ನ ಅವಧಿ ಎಷ್ಟು?\n","Response: ಮಾಸ್ಟರ್ ಡಾಟಾ ಸೈನ್ಸ್ ಕೋರ್ಸ್ನ ಅವಧಿಯು 3 ತಿಂಗಳು (ವಾರದ ದಿನಗಳು) ಅಥವಾ 5 ತಿಂಗಳು (ವಾರಾಂತ್ಯಗಳು).\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"96lRE3D-rLjn"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyM9NMAfe+yC7rKEawpSkkai"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0dbb0c7778204cc5b6eae10ed6e32f56":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_febdf61c8a2b4cbfa377d2e3075b040d","IPY_MODEL_85b7e2b5ce6f4de3a1e4bdfadcc3bf35","IPY_MODEL_8ecc5f48b5fd4d0ab8270f977b7476ac"],"layout":"IPY_MODEL_254ec91c19384f29b70d5cb0f95ab6f5"}},"febdf61c8a2b4cbfa377d2e3075b040d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f28d72e6e40c478da77a925e09401161","placeholder":"​","style":"IPY_MODEL_39d721582a4244729fcf80d7d9fb6225","value":"Batches: 100%"}},"85b7e2b5ce6f4de3a1e4bdfadcc3bf35":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d253f1b29fd74d55be909f5f380686e8","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2d47787269674f7ca80f3a8694d1e1e0","value":4}},"8ecc5f48b5fd4d0ab8270f977b7476ac":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8204ea343f7446228f0a45dea44605dc","placeholder":"​","style":"IPY_MODEL_6c13b891439d459a92fe70c2869c9013","value":" 4/4 [00:00&lt;00:00, 15.42it/s]"}},"254ec91c19384f29b70d5cb0f95ab6f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f28d72e6e40c478da77a925e09401161":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39d721582a4244729fcf80d7d9fb6225":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d253f1b29fd74d55be909f5f380686e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d47787269674f7ca80f3a8694d1e1e0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8204ea343f7446228f0a45dea44605dc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c13b891439d459a92fe70c2869c9013":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}